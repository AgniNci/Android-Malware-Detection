{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b67a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from androguard.misc import AnalyzeAPK\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randrange\n",
    "from IPython.display import clear_output\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "from tensorflow.keras import datasets, layers, models, callbacks\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import hashlib\n",
    "# import keras_tuner as kt\n",
    "# from kerastuner import HyperParameter, HyperParameters\n",
    "# from kerastuner.tuners import RandomSearch, BayesianOptimization, Hyperband\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc39c62",
   "metadata": {},
   "source": [
    "# Extract APKs from compressed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec76374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = glob.glob('../../CIC_Dataset_2019/*.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22592eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75f0713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract all apk files from the compressed folders\n",
    "# for file in files:\n",
    "#     with zipfile.ZipFile(file, 'r') as zip_ref:\n",
    "#             zip_ref.extractall('../../Research/apks/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eaca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of all benign apks\n",
    "# benign_apks = os.listdir(\"../../Research/apks/Benign_2015/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410dcbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename the benign apk files with its md5 digest\n",
    "# for apk in benign_apks:\n",
    "#     md5_digest = md5(\"../../Research/apks/Benign_2015/\" +  apk)\n",
    "#     os.rename(\"../../Research/apks/Benign_2015/\" + apk, \"../../Research/apks/Benign_2015/\" + md5_digest + \".apk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509bd0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'id':'md5', 'is_malicious': 1, 'family_name': 'Adware', 'type':'dowgin' }\n",
    "md5_list = []\n",
    "for root, dirs, files in os.walk('../../Research/apks/', topdown=True):\n",
    "    apk_files = glob.glob(root + '/*.apk')\n",
    "    for file in apk_files:\n",
    "        arr = file[20:].split(\"\\\\\")\n",
    "        md5_list.append({'id' : arr[-1][:-4], 'is_malicious' : 0 , 'type' : arr[-2] if len(arr) == 3 else np.nan , 'label' : arr[0], 'root_path' : root})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720f8146",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_apk = pd.DataFrame(md5_list)\n",
    "df_apk.drop_duplicates(subset=['id'], inplace = True)\n",
    "df_apk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faee2593",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_apk['is_malicious'] = df_apk['label'].apply(lambda x : 0 if \"Benign\" in x else 1)\n",
    "df_apk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc70f96",
   "metadata": {},
   "source": [
    "# Extract opcodes from dex files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9566a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPCODE_FILE_PATH = \"../../Research/nlp_last_experiment/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7525c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(OPCODE_FILE_PATH)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bf214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_md5_list(images_path, all_md5_list):\n",
    "    converted_imges = os.listdir(images_path)\n",
    "    converted_md5s = [md5[:-4] for md5 in converted_imges]\n",
    "    print(len(converted_imges), 'are converted.')\n",
    "\n",
    "    unconverted_md5_list = list(set(all_md5_list) - set(converted_md5s))\n",
    "    print(len(unconverted_md5_list), 'APKs are remaining to be converted.')\n",
    "    return converted_md5s, unconverted_md5_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b5832",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opcode Extraction from dex files\n",
    "def apk_to_opcodes(a, d, dx):\n",
    "    opcodes = []\n",
    "    instructions = []\n",
    "    for m_a in dx.get_methods():\n",
    "        if m_a.is_external():\n",
    "        # External methods do not have opcodes\n",
    "            continue\n",
    "        idx = 0\n",
    "        m = m_a.get_method()\n",
    "        for ins in m.get_instructions():\n",
    "            # You can access the instruction here and do stuff with it... \n",
    "#             print('{:08x}  {:04x}  {:24s} {}'.format(idx, ins.get_op_value( ), ins.get_name(), ins.get_output()))\n",
    "            instructions.append(ins)\n",
    "            opcodes.append(ins.get_name())\n",
    "            idx += ins.get_length()\n",
    "    return opcodes, instructions\n",
    "\n",
    "\n",
    "def apk_to_api_sequences(a, d, dx):\n",
    "    methods = []\n",
    "    for m_a in dx.get_methods():\n",
    "        m = m_a.get_method()\n",
    "        if m_a.is_external():\n",
    "        # External methods do not have opcodes\n",
    "            continue\n",
    "        idx = 0\n",
    "        method_opcodes = []\n",
    "        for ins in m.get_instructions():\n",
    "            # You can access the instruction here and do stuff with it... \n",
    "            method_opcodes.append(ins.get_name( ))\n",
    "            idx += ins.get_length()\n",
    "        methods.append(\"\".join(method_opcodes).strip())\n",
    "    print(f'All Methods Count: {len(methods)}')\n",
    "    unique_methods = list(set(methods))\n",
    "    print(f'All Unique Methods Count: {len(unique_methods)}')\n",
    "    return unique_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575710cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "shas_list = []\n",
    "for index, row in df_apk.iterrows():\n",
    "    shas_list.append(row['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b0a124",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_shas = get_md5_list(OPCODE_FILE_PATH, shas_list)[0]\n",
    "\n",
    "to_be_converted_df = df_apk.copy()\n",
    "to_be_converted_df.set_index('id', inplace = True)\n",
    "to_be_converted_df.drop(converted_shas, axis = 0, inplace=True)\n",
    "to_be_converted_df.reset_index(inplace = True)\n",
    "\n",
    "for index, row in to_be_converted_df.iterrows():\n",
    "    print('Analysing :', row['id'])\n",
    "    \n",
    "    try:\n",
    "        #Androguard\n",
    "        a,d,dx = AnalyzeAPK(os.path.join(row['root_path'] + '/' + row['id'] + '.apk'))\n",
    "        opc = apk_to_api_sequences(a,d,dx)\n",
    "        np.savetxt(os.path.join(OPCODE_FILE_PATH + row['id'] + '.txt'), opc, fmt=\"%s\", delimiter = \" \")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2e3b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a DataFrame for converted apks\n",
    "unconverted_md5s = get_md5_list(OPCODE_FILE_PATH, shas_list)[1]\n",
    "converted_df = df_apk.copy()\n",
    "converted_df.set_index('id', inplace = True)\n",
    "converted_df.drop(unconverted_md5s, axis = 0, inplace=True)\n",
    "converted_df.reset_index(inplace = True)\n",
    "converted_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723bb1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0d32ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_df.drop(columns = ['root_path'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11b7871",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_df['opcode_path'] = converted_df['id'].apply(lambda x : os.path.join(OPCODE_FILE_PATH + x + '.txt'))\n",
    "converted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fa4f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the opcodes\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d842c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sequences_from_text(text_list, num_sequences, seq_size = 420):\n",
    "    sub_sample_sizes = [int(seq_size * 0.4), int(seq_size * 0.3), int(seq_size * 0.2), int(seq_size * 0.1)]\n",
    "    #print(f'Sub Sample Sizes : {sub_sample_sizes}')\n",
    "    chunks = np.array_split(text_list, len(sub_sample_sizes))\n",
    "    sequences = []\n",
    "    for num in range(num_sequences):\n",
    "        np.random.shuffle(chunks)\n",
    "        seq = []\n",
    "        for i in range(len(sub_sample_sizes)):\n",
    "            seq += np.ndarray.tolist(chunks[i][:sub_sample_sizes[i]])\n",
    "        sequences.append(\" \".join(seq))\n",
    "        \n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d326ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing data for lstm\n",
    "# opcode_list = []\n",
    "# corpus = \"\"\n",
    "def create_records():\n",
    "    try:\n",
    "        for index, row in converted_df.iterrows():\n",
    "            print(f'Index : {index}')\n",
    "            with open(row[\"opcode_path\"], 'r') as f:\n",
    "                text = f.read()\n",
    "                unique_opseq_list = text.split(\"\\n\")\n",
    "                samples = []\n",
    "                if len(unique_opseq_list) > 600:\n",
    "                    samples += sample_sequences_from_text(unique_opseq_list, 2)\n",
    "                else:\n",
    "                    samples.append(text.replace(\"\\n\", \" \"))\n",
    "                count = 0\n",
    "                for sample in samples:\n",
    "                    if count > 0:\n",
    "                        yield {\"file_name\" : f'{f.name}{count}', \"opcode\" : sample, \"is_malicious\" : row[\"is_malicious\"]}\n",
    "                    else:\n",
    "                        yield {\"file_name\" : f.name, \"opcode\" : sample, \"is_malicious\" : row[\"is_malicious\"]}\n",
    "                    count += 1\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d9f495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opcode_list = []\n",
    "for record in create_records():\n",
    "    opcode_list.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5968ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "opcode_df = pd.DataFrame(opcode_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026ac5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "opcode_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377a7e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "opcode_df[\"is_malicious\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e14c9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "opcode_df.to_csv(\"opcode.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b34626",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(opcode_df[\"opcode\"], opcode_df[\"is_malicious\"], random_state = 100, train_size = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc6dedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaf29d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d6ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(input = 'content', ngram_range = (1,2), min_df = 5, token_pattern = r\"[a-z0-9-\\/]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af7873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9135869",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized = vectorizer.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78f7cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc5b642",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df42040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_vectorized.toarray()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed0a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79057ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized = X_train_vectorized / X_train_vectorized.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ca0a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_vectorized.toarray()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0096e217",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a121be13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3dd1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47ad658",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "# predictions = model.predict(X_test_vectorized/X_test_vectorized.max())\n",
    "predictions = model.predict(X_test_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8778516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d104a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a608c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC/ROC score is : \", roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbffe93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "sorted_coef_index = model.coef_[0].argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0148e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_names[sorted_coef_index[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46f03d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_names[sorted_coef_index[:-11:-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1b9a65",
   "metadata": {},
   "source": [
    "# Implementing LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0fcdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \" \".join(opcode_df[\"opcode\"].tolist())\n",
    "vocab = sorted(list(set(corpus.split(\" \"))))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b684f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32170b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(opcode_df[\"opcode\"], opcode_df[\"is_malicious\"], random_state = 56, train_size = 0.7)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5048c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d15fc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of the model\n",
    "vocab_size = len(vocab)\n",
    "oov_tok = ''\n",
    "embedding_dim = 100\n",
    "max_length = 200 # choose based on statistics, for example 150 to 200\n",
    "padding_type='post'\n",
    "trunc_type='post'\n",
    "# tokenize sentences\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok, filters=\"\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "# convert train dataset to sequence and pad sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "train_padded = pad_sequences(train_sequences, padding='post', maxlen=max_length)\n",
    "# convert Test dataset to sequence and pad sequences\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "test_padded = pad_sequences(test_sequences, padding='post', maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96731a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbb752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, input_length = max_length))\n",
    "# LSTM\n",
    "model.add(layers.LSTM(128))\n",
    "\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86357c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(train_padded, y_train, validation_data=(test_padded, y_test),epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b97321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert to a sequence\n",
    "# sequences = tokenizer.texts_to_sequences(sentence)\n",
    "# # pad the sequence\n",
    "# padded = pad_sequences(sequences, padding='post', maxlen=max_length)\n",
    "prediction = model.predict(test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc1540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get labels based on probability 1 if p>= 0.5 else 0\n",
    "pred_labels = []\n",
    "for pred in prediction:\n",
    "    if pred > 0.5:\n",
    "        pred_labels.append(1)\n",
    "    else:\n",
    "        pred_labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9015fdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89d2c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AUC/ROC score is : \", roc_auc_score(y_test, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b2b17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09af336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(y_test, y_pred) :\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    print('Recall: %f' % recall)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print('F1 score: %f' % f1)\n",
    "\n",
    "\n",
    "def plot_model_performance(history):\n",
    "    # plot loss during training\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.title('Loss')\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "#     plt.savefig(\"./loss_vs_epoch.png\")\n",
    "\n",
    "    # plot accuracy during training\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.title('Accuracy')\n",
    "    plt.plot(history.history['accuracy'], label='train')\n",
    "    plt.plot(history.history['val_accuracy'], label='test')\n",
    "    plt.grid(True)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846453fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores(y_test, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d4531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_performance(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727debbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model(model, \"../lstm/opcode_nlp_novel_sequencing_13122022_0534.h5\", save_format = \"h5\")\n",
    "model.save(\"../lstm/opcode_nlp_novel_sequencing_13122022_0534.h5\")\n",
    "# save_model(model, \"../lstm/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d3ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, input_length = max_length))\n",
    "# LSTM\n",
    "model.add(layers.Bidirectional(layers.LSTM(64)))\n",
    "\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3f239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(train_padded, y_train, validation_data=(test_padded, y_test),epochs=30, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28db2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the model\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.2, 1])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e3bcb0",
   "metadata": {},
   "source": [
    " # Embedding layer without any max sequence length and a GlobalAveragePooling1D layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68843205",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to investigate further"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7190ad",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bca3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9cd680",
   "metadata": {},
   "outputs": [],
   "source": [
    "opcode_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6358862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, CoherenceModel\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ffc6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_input = [x.split(\" \") for x in X_train.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085c5d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec Vectorization\n",
    "w2v = Word2Vec(embedding_input, vector_size=100,\n",
    "                                   window=5,\n",
    "                                   min_count=2,\n",
    "                                   workers = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f63279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5d539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(w2v.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24306b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of the model\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)\n",
    "oov_tok = ''\n",
    "embedding_dim = 100\n",
    "max_length = 320 # choose based on statistics, for example 150 to 200\n",
    "padding_type='post'\n",
    "trunc_type='post'\n",
    "# tokenize sentences\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok, filters = \"\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "# convert train dataset to sequence and pad sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "train_padded = pad_sequences(train_sequences, padding='post', maxlen=max_length)\n",
    "# convert Test dataset to sequence and pad sequences\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "test_padded = pad_sequences(test_sequences, padding='post', maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da87fd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dee75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70927cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a737c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = w2v.wv\n",
    "word_vectors.save(\"word2vec.wordvectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9e9bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff63c7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04da6cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = KeyedVectors.load(\"word2vec.wordvectors\", mmap = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ab3c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing embedding matrix\n",
    "nb_words = min(vocab_size, len(word_index))+1\n",
    "# nb_words = len(word_index)\n",
    "embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if word in wv.index_to_key:\n",
    "        embedding_matrix[i] = wv.get_vector(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f4d400",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a9ca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "vocab_size = nb_words\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, weights = [embedding_matrix], input_length = max_length, trainable = False))\n",
    "# LSTM\n",
    "model.add(layers.Bidirectional(layers.LSTM(64)))\n",
    "\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dropout(0.4))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425abf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(i.shape, i.dtype) for i in model.inputs]\n",
    "[print(o.shape, o.dtype) for o in model.outputs]\n",
    "[print(l.name, l.input_shape, l.dtype) for l in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8c8751",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449fad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(train_padded, y_train, validation_data=(test_padded, y_test),epochs=10, verbose=2, callbacks=[es_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5aae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the model\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.2, 1])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e91fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    hp_dense_layers = hp.Int(\"dense_layers\", 1, 3, default=3)\n",
    "    hp_dropout_rate=hp.Choice(\"dropout_rate\", values=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5]) \n",
    "    hp_activation=hp.Choice('activation', values=['relu', 'tanh'])\n",
    "    hp_optimizer = hp.Choice('optimizer', values=['adam', 'SGD', 'rmsprop'])\n",
    "    optimizer = tf.keras.optimizers.get(hp_optimizer)\n",
    "    optimizer.learning_rate = hp.Choice(\"learning_rate\", [0.1, 0.01, 0.001], default=0.01)\n",
    "    inputs = tf.keras.Input(shape=(None, 320))\n",
    "    x=inputs\n",
    "   \n",
    "    x = layers.Embedding(vocab_size, embedding_dim, weights = [embedding_matrix], input_length = max_length, trainable = False)(x)\n",
    "# LSTM\n",
    "    x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "    for i in range(hp_dense_layers):\n",
    "        \n",
    "        x = layers.Dense(\n",
    "            units=hp.Int(\"units_\" + str(i), 4, 64, step=4, default=8),\n",
    "            activation=hp_activation,\n",
    "            padding=\"same\",\n",
    "        )(x)\n",
    "        x = tf.keras.layers.Dropout(hp_dropout_rate)(x)\n",
    "    \n",
    "    # The last layer contains 10 unitsfor the number of classes.\n",
    "    outputs = tf.keras.layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53bc8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=10,\n",
    "    hyperband_iterations=2,\n",
    "    overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Research",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
